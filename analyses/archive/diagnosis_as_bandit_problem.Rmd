---
title: "Developing Disease Understanding"
author: "Rachael Caelie (Rocky) Aikens"
date: "9/30/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE, fig.align = "center", fig.height = 3)
library(confirmationBias)
library(tidyverse)
library(ggpubr)
library(knitr)
library(contextual)
library(binom)

theme_set(theme_light())
```

# Background

Now we'd like to ask a new question: how do these biases arise in the first place?

To start to think about this, let's imagine a toy example.  We discover a totally new disease in the population.  We don't know what the prevalence of the disease is in different demographic groups, and we don't have a complete understanding of the symptoms.  What we do have is 100% reliable diagnostic test. However, we don't have enough test kits to screen everyone.  How do doctors choose who to test?  How does this affect what we learn about the disease and what we do not?

An important thing to keep in mind is that research (understanding the underlying disease prevalence and presentation) is *not* the primary goal of the health care professionals who select who is tested and who is not.  Their primary goal is to find people with the disease as efficiently as possible. In essense, they are trying to use limited testing resources to make as many positive diagnoses as possible.

# A Multi-Armed Bandit Problem

With some simplifications, we might describe this is as a multi-armed bandit problem. Suppose we can represent the patient population as $d$ distinct groups, defined by symptoms, demographics, or a combination (e.g. a group might be "Black adult males with chest pain" or "Asymptomatic Asians, ages 10-17").  Each testing opportunity, the health care provider must decide which person gets a test next. Their goal is to identify as many positive cases as possible. They get a reward of 1 for every positive test result and a reward of zero for every negative test result.

There are several well-articulated strategies for addressing a multi-armed bandit problem.  In practice, doctors probably don't perfectly follow any of them.  But let's consider a few cases so we can understand the problems that arise when optimizing for reward can go awry.

## Set up

For our patient groups, let's imagine 10 total groups, defined by combinations of two sexes (M, F) and five symptoms (chest pain, shortness of breath, cold sweat, fatigue, and nausea).  The underlying disease prevalence for each group is shown below:

```{r}
underlying_process <- tibble(group_id = 1:10, 
                             group = c("Male, chest pain",
                                   "Male, short of breath",
                                   "Male, cold sweat",
                                   "Male, extreme fatigue",
                                   "Male, nausea",
                                   "Female, chest pain",
                                   "Female, short of breath",
                                   "Female, cold sweat",
                                   "Female, extreme fatigue",
                                   "Female, nausea"),
                             sex = rep(c("Male", "Female"), each = 5),
                             symptom = rep(c("chest pain", "short of breath", "cold sweat", "fatigue", "nausea"), 2),
                         prevalence = c(0.15, 0.06, 0.02, 0.001, 0.001,
                                        0.001, 0.001, 0.02, 0.06, 0.15))
ggplot(underlying_process, aes(x = symptom, y = prevalence, group = sex, fill = sex)) +
  geom_col( position = "dodge") +
  facet_wrap(~ sex) +
  scale_fill_brewer(palette = "Set1") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  ylab("True Disease Prevalence")+
  xlab("Symptom")
```

Notice that females and males have the same overall disease prevalence, but they have different symptoms that are most suggestive of disease.

## A simple solution: Epsilon Greedy

First, let's imagine (although this is somewhat unlikely) that doctors follow an epsilon-greedy strategy. That is, at each testing opportunity, they will prefer to test someone from the patient group with the highest observed positive test rate so far.  However, with some relatively low probability $\epsilon$, they will choose which group to test entirely at random.

```{r}
set.seed(123)
horizon            <- 500
weights            <- underlying_process$prevalence

policy             <- EpsilonGreedyPolicy$new(epsilon = 0.1)
bandit             <- BasicBernoulliBandit$new(weights = weights)
agent              <- Agent$new(policy,bandit, "EG")
simulator          <- Simulator$new(agents      = agent,
                                    horizon     = horizon,
                                    set_seed    = 8,
                                    save_theta  = TRUE,
                                    simulations = 1)
simEG <- simulator$run()

df <- simEG$get_data_frame()
```

The plot below shows the patient groups that are tested from for the first 500 tests.  

```{r}
plt_df <- df %>%
  left_join(underlying_process, by = c("choice" = "group_id"))

plt_df %>%
  ggplot(aes(x = t, y = group, color = sex)) + geom_point(alpha = 0.5) +
  scale_color_brewer(palette = "Set1") +
  ylab("Patient selected for testing") +
  xlab("Rounds of testing")
```

There's a lot to unpack here.  

First, notice that for over 200 rounds of testing, the algorithm hasn't even selected a patient group with the highest possible prevalence. Instead of selecting "Male, chest pain" or "Female Nausea" (which each have a disease probability of 15%), testing is focused on females with extreme fatigue.  This is a well-understood problem with epsilon-greedy solutions to a multi-armed bandit problem: greedily resampling from groups which achieved early successes can blind the agent from other groups with a higher success rate. Said more appropriately for this eproblem: testing only the people with the highest known disease prevalence thus far can mean that other high-prevalence groups are not discovered. This "jumping to incorrect conclusions" is more likely when epsilon is small and when fewer rounds of testing are done.

The plot below shows the data in a slightly different way: the estimated prevalence of the disease in each group as sampling is done.

```{r}
unnest_auto(df, theta) %>%
  unnest_longer(mean) %>%
  mutate(group_id = rep(1:10, 500)) %>%
  select(mean, group_id, t...1)%>%
  rename(t = t...1) %>%
  left_join(underlying_process) %>%
  ggplot(aes(x = t, y = mean, group = group_id, color = group)) + geom_line()
```

However, there is something noticeably inaccurate about this way of modeling doctor behavior. Males with chest pain and females with nausea have equal underlying rates of the disease.  When and if enough sampling is done that this is discovered, doctors should want to test each of these two high-likelihood-of-disease groups with at the same rate.  A greedy agent doesn't really care about testing these groups at the same rate, as long as it is always testing a group with a high rate of underlying disease.  In essence, if the greedy agent knew the underlying disease probabilities, they would just as happily test *only* females with nausea, and ignore all men with chest pain.  This isn't a particularly doctorly thing to do.

## UCB

Let's give doctors more credit.  Let's suppose they have some underlying understanding that there is uncertainty in their estimates of prevalence in a group when they have only run a small number of tests on individuals from that group.  Let's also suppose that they're extra smart and they know that using the upper confidence bound to select whom to test next is a good policy (i.e. they use the UCB algorithm).  In the UCB approach, the agent keeps track not only of success rates but of the confidence bounds around those success rates.  Each round, they select whichever group has the highest upper confidence bound on their prevalence.  Intuitively, this approach favors options with high success rates, and options that have not been explored much.

```{r}
set.seed(123)
horizon            <- 500
weights            <- underlying_process$prevalence

policy             <- UCB1Policy$new()
bandit             <- BasicBernoulliBandit$new(weights = weights)
agent              <- Agent$new(policy,bandit, "UCB")
simulator          <- Simulator$new(agents      = agent,
                                    horizon     = horizon,
                                    set_seed    = 8,
                                    save_theta  = TRUE,
                                    simulations = 1)
simEG <- simulator$run()


df <- simEG$get_data_frame()
```

```{r}
plt_df <- df %>%
  left_join(underlying_process, by = c("choice" = "group_id"))

plt_df %>%
  ggplot(aes(x = t, y = group, color = sex)) + geom_point(alpha = 0.5) +
  scale_color_brewer(palette = "Set1") +
  ylab("Patient selected for testing") +
  xlab("Rounds of testing")
```

```{r}
unnest_auto(df, theta) %>%
  unnest_longer(mean) %>%
  mutate(group_id = rep(1:10, 500)) %>%
  select(mean, group_id, t...1)%>%
  rename(t = t...1) %>%
  left_join(underlying_process) %>%
  ggplot(aes(x = t, y = mean, group = group_id, color = group)) + geom_line()
```

Now, doctors do a lot better.  After 500 rounds of testing they still don't have the relative disease rates in each group exactly right, but they don't quickly jump to conclusions the way they did when the epsilon-greedy algorithm was used.

But... maybe this gives doctors *too much* credit.  Do all doctors really precisely think about the level of uncertainty in the prevailing literature when they do their diagnosis?  Probably not.  Are they mindful of the fact that using the upper confidence bound on estimated success rate is a well-established solution to the multi-armed-bandit problem?  Doubtful.

What doctors actually do is probably not exactly greedy, nor is it based on exact uncertainty calculations.

## Thompson Sampling

Thompson Sampling may be a better model for what health care providers actually do, since it is a Bayesian decision making process.  They begin with an uniform prior - all groups are equally likely to have the disease - and update their prior as they make observations.  The choice of who to test next is directed by sampling from their developing posterior distribution.  This means that groups tend to be sampled in accordance to the agent's current belief about who is likely to have the disease.

```{r}
set.seed(123)
horizon            <- 500
weights            <- underlying_process$prevalence

policy             <- ThompsonSamplingPolicy$new()
bandit             <- BasicBernoulliBandit$new(weights = weights)
agent              <- Agent$new(policy,bandit, "UCB")
simulator          <- Simulator$new(agents      = agent,
                                    horizon     = horizon,
                                    set_seed    = 8,
                                    save_theta  = TRUE,
                                    simulations = 1)
simEG <- simulator$run()

df <- simEG$get_data_frame()
```

```{r}
plt_df <- df %>%
  left_join(underlying_process, by = c("choice" = "group_id"))

plt_df %>%
  ggplot(aes(x = t, y = group, color = sex)) + geom_point(alpha = 0.5) +
  scale_color_brewer(palette = "Set1") +
  ylab("Patient selected for testing") +
  xlab("Rounds of testing")
```

```{r}
plt_data <- unnest_auto(df, theta) %>%
  unnest_longer(succes) %>%
  mutate(group_id = rep(1:10, 500)) %>%
  select(succes, group_id, t...1) %>%
  rename(t = t...1) 
  

fail_col <- unnest_auto(df, theta) %>%
  unnest_longer(failure) %>%
  pull(failure)

plt_data %>%
  mutate(failure = fail_col) %>%
  left_join(underlying_process) %>%
  mutate(mean = succes/(succes + failure)) %>%
  ggplot(aes(x = t, y = mean, group = group_id, color = group)) + geom_line()
```

Thompson sampling does fairly well.  However, by 500 tests there are still some prevalences that aren't estimated particularly well.  If the doctors continue acting as perfect bayesian agents, they will eventually converge to a correct understanding of the disease.  

But, do doctors really act as perfect bayesian agents?  Or is there some point at which "male chest pain and shortness of breath" and "female nausea and fatigue" becomes part of the informal definition of the disease, and the possibility of other presentations is forgotten?  What if that fixation happened early in the disease process?  What would happen in this simulation if, at 200 tests, we stopped considering our uniform prior and ceased searching for other possible presentations of the disease? We might begin to define this disease as a illness of women manifesting with symptoms of nausea, in spite of the fact that it has equal prevalence - and divergent presentation - in men and women.



# A Contextual Bandit Problem

Now let's try formulating this as a modification on a contextual bandit problem.  As before, we'll have 10 different types of patients, but we'll change problem slightly.  Now, the patients appear at the hospital in a random order, and the doctor must sequentially decide whether to test the patients as they arrive.  If the doctor tests the patient and they test positive, they recieve a reward of 1.  However, for every patient they choose not to test, they get a small constant reward for not using the testing resources. 

## LinUCB

A standard generalization of UCB to the contextual bandit is linUCB, which estimates the probability of reward for a patient based on a linear model.

```{r}
underlying_process <- tibble(group = 1:3, prevalence = c(0.15, 0.06, 0.001))
notest_reward <- 0.02
```


```{r}
ContextualDiagnosisBandit <- R6::R6Class(
  inherit = Bandit,
  
  class = FALSE,
  public = list(
    weights = NULL,
    class_name = "ContextualDiagnosisBandit",
    initialize = function(prevalences, notest_reward) {
      self$d <- length(prevalences) # d features
      self$k <- 2 # k arms
      self$weights <- matrix(c(prevalences, rep(notest_reward, self$d)), nrow = self$d) # d x k weight matrix
    },
    get_context = function(t) {
      # generate d dimensional feature vector, one random feature active at a time
      Xa <- sample(c(1,rep(0,self$d-1)))
      context <- list(
        X = Xa,
        k = self$k,
        d = self$d
      )
    },
    get_reward = function(t, context, action) {
      # which arm was selected?
      arm <- action$choice
      # d dimensional feature vector for chosen arm
      Xa <- context$X
      # weights of active context
      weight <- Xa %*% self$weights
      # assign rewards for active context with weighted probs
      rewards <- as.double(c(weight[1] > runif(1), weight[2]))
      optimal_arm <- which_max_tied(weight)
      reward <- list(
        reward = rewards[arm],
        optimal_arm = optimal_arm,
        optimal_reward = weight[optimal_arm]
      )
    }
  )
)
```

```{r}
horizon       <- 150

simulations = 1

bandit        <- ContextualDiagnosisBandit$new(prevalences = underlying_process$prevalence,
                                               notest_reward = notest_reward)

# Linear CMAB policies comparison

agents <- Agent$new(LinUCBDisjointOptimizedPolicy$new(0.6), bandit, "LinUCB")

simulation     <- Simulator$new(agents, horizon, simulations, do_parallel = TRUE)

history        <- simulation$run()

df <- history$get_data_frame()
```


```{r}
plt_df <- df %>%
  mutate(prevalence = ifelse(optimal_arm == 2, 0.001, optimal_reward)) %>%
  left_join(underlying_process) %>%
  rowwise() %>%
  mutate(disease = ifelse(choice == 1, reward, prevalence > runif(1))) %>%
  ungroup() %>%
  mutate(tested = choice == 1) %>%
  mutate(status = case_when(
    tested & disease ~ "tested, positive",
    tested & !disease ~ "tested, negative",
    !tested & disease ~ "untested, positive",
    !tested & !disease ~ "untested, negative")) %>%
  mutate(status = factor(status, levels = c("untested, negative", "untested, positive", "tested, negative", "tested, positive")))

plt_df %>%
  ggplot(aes(x = t, y = group, color = status)) + geom_point(alpha = 0.5)  +
  scale_color_brewer(palette = "Set1")
```

# The difficulty of estimating true prevalence - even with a perfect diagnostic test

There is something important about the way we think about the "disease rate" in a group of people.  In the simulations above, our understanding of the disease is shaped by the **positive test rate** - Of the people that were tested, how many tested positive?  In these simulations, we ensure there are no selection processes biasing who gets tested *within* a patient group, so the positive test rate is in fact a good estimate for the prevalence within that group.  In reality, however, selection processes are likely (e.g. only men with insurance coverage get the test, only women with the most severe fatigue go to the hospital), and this may bias our estimates of disease prevalence if there is within-group variation that corresponds to both the selection process and the probabilty of the disease.  

Another - possibly *more* misleading - metric that might be used is the **overall rate of diagnosis** - i.e. of *all* of the people in this group (assessed or unassesssed), how many tested positive?  This is misleading because it makes the strong assumption that all of the people who were never tested never had the disease. In spite of this, overall rate of diagnosis does sometimes factor into decision making as a proxy for prevalence - consider for example the way that Americans assumed that the prevalence of COVID-19 was very low in their community early in the pandemic, when in fact there were probably many people with COVID-19 who were simply untested and misdiagnosed or *assumed* healthy. When the overall rate of diagnosis is used as a proxy for prevalence, *who* is tested becomes especially important, because untested people are assumed healthy by default.  Our understanding of prevalence will be better in groups that we choose to test often, and it may be a drastic underestimate in the groups that we never test.

Consider that contextual bandit simulation with linUCB solvers.  The tables below show the positive test rate, the overall rate of diagnosis, and the true prevalence summarized over 100 simulations of the first 150 tests in the disease discovery process.  In this case, there is still no selection bias determining who is tested *within* a group, so the positive test rate is a good estimate of the true prevalence.  However, the overall population rate consistently underestimates the prevalence within groups.  Another imporant characteristic to note is that the cost of testing is an important factor.  When the reward for not testing a patient is increased, the overall diagnosis rate becomes a more severe underestimate of the true prevalence.

```{r}
notest_reward <- 0.02

horizon       <- 150

simulations = 100

bandit        <- ContextualDiagnosisBandit$new(prevalences = underlying_process$prevalence,
                                               notest_reward = notest_reward)

# Linear CMAB policies comparison

agents <- Agent$new(LinUCBDisjointOptimizedPolicy$new(0.6), bandit, "LinUCB")

simulation     <- Simulator$new(agents, horizon, simulations, do_parallel = TRUE)

history        <- simulation$run()

df <- history$get_data_frame()
```

```{r}

plt_df <- df %>%
  mutate(prevalence = ifelse(optimal_arm == 2, 0.001, optimal_reward)) %>%
  left_join(underlying_process) %>%
  rowwise() %>%
  mutate(disease = ifelse(choice == 1, reward, prevalence > runif(1))) %>%
  ungroup() %>%
  mutate(tested = choice == 1) %>%
  mutate(status = case_when(
    tested & disease ~ "tested, positive",
    tested & !disease ~ "tested, negative",
    !tested & disease ~ "untested, positive",
    !tested & !disease ~ "untested, negative")) %>%
  mutate(status = factor(status, levels = c("untested, negative", "untested, positive", "tested, negative", "tested, positive")))


plt_df %>%
  group_by(group) %>%
  summarize(test_rate = sum(tested)/n(),
            positive_test_rate = sum(disease & tested)/sum(tested),
            diagnosis_rate = mean(reward == 1),
            prevalence = mean(disease)) %>%
  kable(digits = 2, col.names = c("Group", "Proportion tested", "Positive test rate", "Diagnosis rate", "True prevalence"), caption = "Observed statistics when the reward for not testing is 0.02")
```

```{r}
notest_reward <- 0.05

horizon       <- 150

simulations = 100

bandit        <- ContextualDiagnosisBandit$new(prevalences = underlying_process$prevalence,
                                               notest_reward = notest_reward)

# Linear CMAB policies comparison

agents <- Agent$new(LinUCBDisjointOptimizedPolicy$new(0.6), bandit, "LinUCB")

simulation     <- Simulator$new(agents, horizon, simulations, do_parallel = TRUE)

history        <- simulation$run()

df <- history$get_data_frame()
```

```{r}

plt_df <- df %>%
  mutate(prevalence = ifelse(optimal_arm == 2, 0.001, optimal_reward)) %>%
  left_join(underlying_process) %>%
  rowwise() %>%
  mutate(disease = ifelse(choice == 1, reward, prevalence > runif(1))) %>%
  ungroup() %>%
  mutate(tested = choice == 1) %>%
  mutate(status = case_when(
    tested & disease ~ "tested, positive",
    tested & !disease ~ "tested, negative",
    !tested & disease ~ "untested, positive",
    !tested & !disease ~ "untested, negative")) %>%
  mutate(status = factor(status, levels = c("untested, negative", "untested, positive", "tested, negative", "tested, positive")))


plt_df %>%
  group_by(group) %>%
  summarize(test_rate = sum(tested)/n(),
            positive_test_rate = sum(disease & tested)/sum(tested),
            diagnosis_rate = mean(reward == 1),
            prevalence = mean(disease)) %>%
  kable(digits = 2, col.names = c("Group", "Proportion tested", "Positive test rate", "Diagnosis rate", "True prevalence"), caption = "Observed statistics when the reward for not testing is 0.05")
```
